{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZYQL1VphtO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization,LeakyReLU,PReLU,Dense,Flatten,add,Conv2DTranspose,UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input\n",
        "import numpy as np\n",
        "from tensorflow.image import resize,ResizeMethod\n",
        "import tensorflow.keras.backend as K\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RDgi_7ohxmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "file_path = '/content/gdrive/My Drive/Colab Notebooks/SRGAN'\n",
        "a=os.listdir('/content/gdrive/My Drive/Colab Notebooks/SRGAN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H-npJf8iEfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model architecture\n",
        "def D_block(D,output_size,stride):\n",
        "  D = Conv2D(filters=output_size, kernel_size=3,strides = stride, padding=\"same\")(D)\n",
        "  D = BatchNormalization()(D)\n",
        "  D = LeakyReLU()(D)\n",
        "  return D\n",
        "\n",
        "def res_block(G):\n",
        "  res = G\n",
        "  G = Conv2D(filters=64,kernel_size=3,strides=1,padding=\"same\")(G)\n",
        "  G = BatchNormalization()(G)\n",
        "  G = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None,shared_axes=[1,2])(G)\n",
        "  G = Conv2D(filters=64,kernel_size=3,strides=1,padding=\"same\")(G)\n",
        "  G = BatchNormalization()(G)\n",
        "  G = add([res,G])\n",
        "  return G\n",
        "\n",
        "class GAN:\n",
        "  def __init__(self,image_shape,noise_shape):\n",
        "    self.image_shape = image_shape\n",
        "    self.noise_shape = noise_shape\n",
        "    self.D = self.discriminator()\n",
        "    self.G = self.generator()\n",
        " \n",
        "  def discriminator(self):\n",
        "    input = Input(shape=self.image_shape)\n",
        "    D = Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(input)\n",
        "    D = LeakyReLU(alpha=0.2)(D)\n",
        "    D = D_block(D,64,2)\n",
        "    D = D_block(D,128,1)\n",
        "    D = D_block(D,128,2)\n",
        "    D = D_block(D,256,1)\n",
        "    D = D_block(D,256,2)\n",
        "    D = D_block(D,512,1)\n",
        "    D = D_block(D,512,2)\n",
        "\n",
        "    D = Flatten()(D)\n",
        "    D = Dense(1024)(D)\n",
        "    D = LeakyReLU(alpha=0.2)(D)\n",
        "\n",
        "    D = Dense(1,activation = 'sigmoid')(D)\n",
        "    D_model = tf.keras.Model(inputs=input,outputs=D)\n",
        "\n",
        "    return D_model\n",
        "\n",
        "\n",
        "  def generator(self):\n",
        "    input = Input(shape=self.noise_shape)\n",
        "    G = Conv2D(filters=64,kernel_size=9,strides=1,padding=\"same\")(input)\n",
        "    G = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(G)\n",
        "\n",
        "    res = G\n",
        "\n",
        "    for index in range(16):\n",
        "      G = res_block(G)\n",
        "    \n",
        "    G = Conv2D(filters=64,kernel_size=3,strides=1,padding=\"same\")(G)\n",
        "    G = BatchNormalization()(G)\n",
        "    G = add([res,G])\n",
        "\n",
        "    for index in range(2):\n",
        "      G = Conv2D(filters=256,kernel_size=3,strides=1,padding=\"same\")(G)\n",
        "      #G = Conv2DTranspose(filters= ,kernel_size = ,stride = ,padding = )(G)\n",
        "      G = UpSampling2D(size=(2,2))(G)\n",
        "      G = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(G)\n",
        "    \n",
        "    G = Conv2D(filters=3,kernel_size=9,strides=1,padding=\"same\")(G)\n",
        "\n",
        "    G_model = tf.keras.Model(inputs=input,outputs=G)\n",
        "    return G_model\n",
        "\n",
        "def combined(discriminator,generator):\n",
        "  gan_input = Input(shape=(None,None,3))\n",
        "  SR = generator(gan_input)\n",
        "  gan_output = discriminator(SR)\n",
        "  gan = tf.keras.Model(inputs=gan_input,outputs=[SR,gan_output])\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate = 1e-4,\n",
        "      decay_steps=1e5,\n",
        "      decay_rate=0.1)\n",
        "  adam = Adam(learning_rate = lr_schedule)\n",
        "  gan.compile(loss=[content_loss,'binary_crossentropy'], loss_weights=[1,1e-3],optimizer='adam')\n",
        "\n",
        "  return gan\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkK3ug2Hi89f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vgg19 block5_conv4의 feature map의 MSE 를 loss로 이용했다. content loss\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(256,256,3))\n",
        "vgg19.trainable = False\n",
        "for layer in vgg19.layers:\n",
        "  layer.trainable = False\n",
        "feature_map = tf.keras.Model(inputs = vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
        "feature_map.trainable = False\n",
        "\n",
        "@tf.function()\n",
        "def content_loss(y_true, y_pred):\n",
        "  return K.mean(K.square(feature_map(y_true) - feature_map(y_pred)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LO6UzGvjRsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터 준비\n",
        "#hr_train = 이미지파일 load\n",
        "#lr_train = 이미지 파일 load\n",
        "#hr이미지는 [-1,1] 범위를 갖도록\n",
        "hr_train_norm = hr_train.astype(np.float32)/127.5 - 1\n",
        "#lr 이미지는 [0,1] 범위를 갖도록\n",
        "lr_train_norm = lr_train.astype(np.float32)/255.0\n",
        "'''\n",
        "def hr_to_lr(hr):\n",
        "  lr = []\n",
        "  for i in range(len(hr)):\n",
        "    lr.append(resize(hr[i],[64,64],method=ResizeMethod.BICUBIC).numpy())\n",
        "\n",
        "  return lr\n",
        "\n",
        "def data_preprocess(hr,lr):\n",
        "  #hr은 [-1,1]을 갖도록\n",
        "  for i in range(0,800):\n",
        "    hr[i] = (hr[i]/127.5)-1\n",
        "    lr[i] = (lr[i]/255.0)\n",
        "  #ㅣr은 [0,1]을 갖도록\n",
        "  \n",
        "  return hr, lr\n",
        "\n",
        "hr_train = np.load(file_path+'/hr_train.npy')\n",
        "lr_train = np.load(file_path+'/lr_train.npy')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSuNF8a0jPlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_shape=(64,64,3)\n",
        "image_shape=(256,256,3)\n",
        "\n",
        "model = GAN(image_shape,noise_shape)\n",
        "\n",
        "#pre-trained SRResNet(generator)\n",
        "#논문에서는 pretrain iteration 1e6\n",
        "generator = model.G\n",
        "adam = Adam(learning_rate=1e-4,beta_1= 0.9)\n",
        "generator.compile(loss=content_loss,optimizer=adam)\n",
        "discriminator = model.discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer = adam)\n",
        "discriminator.trainable = False\n",
        "if os.path.isfile(file_path + '/generator_weights.h5'):\n",
        "  generator.load_weights(file_path + '/generator_weights.h5')\n",
        "\n",
        "for i in range(1000):\n",
        "  print('iterate : ', i)\n",
        "  generator.fit(x=lr_train_norm,y=hr_train_norm,batch_size=16,epochs=100,verbose=1)\n",
        "  generator.save_weights(file_path + '/generator_weights.h5')\n",
        "  sr_predict = generator.predict(np.array([lr_train_norm[0]]))\n",
        "  plt.imshow((sr_predict+1)/2)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAZNuUTjvbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SRGAN training\n",
        "noise_shape=(64,64,3)\n",
        "image_shape=(256,256,3)\n",
        "\n",
        "gan = combined(discriminator,generator)\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "# 1e5 동안 learning rate = 1e-4\n",
        "# 1e5 동안 learning rate = 1e-5\n",
        "for i in range(1,epochs+1):\n",
        "  print('#'*15, \"Epochs : \", i, '#'*15)\n",
        "  for count in range(0,round(hr_train_norm.shape[0]/batch_size)):\n",
        "    #끝부분에서 이상하게 되겠는데\n",
        "\n",
        "    batch=np.random.randint(0,hr_train_norm.shape[0],size=batch_size)\n",
        "    hr_batch = hr_train_norm[batch]\n",
        "    lr_batch = lr_train_norm[batch]\n",
        "    \n",
        "\n",
        "    sr_batch = generator.predict(lr_batch)\n",
        "\n",
        "    #discriminator 학습\n",
        "    discriminator.trainable = True\n",
        "    d_loss_real = discriminator.train_on_batch(hr_batch,np.ones((batch_size,1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(sr_batch,np.zeros((batch_size,1)))\n",
        "\n",
        "    discriminator.trainable = False\n",
        "    \n",
        "    \n",
        "    #discriminator의 loss로 다시 generator 학습\n",
        "    # D 에서 label이 1 이라고 했을 떄의 loss를 구해서 그 차이를 줄이는 방향으로 G를 학습\n",
        "    gan_loss = gan.train_on_batch(lr_batch,[hr_batch,np.ones((batch_size,1))])\n",
        "    \n",
        "\n",
        "  print('d_loss_real : ',d_loss_real)\n",
        "  print('d_loss_fake : ',d_loss_fake)\n",
        "  print('gan_loss : ',gan_loss)\n",
        "\n",
        "  generator.save_weights(file_path + '/weights/gen_weights.h5')\n",
        "  discriminator.save_weights(file_path + '/weights/dis_weights.h5')\n",
        "  gan.save_weights(file_path + '/weights/gan_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}